%%%%%%%% ICML 2023 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{csquotes}
\usepackage{adjustbox}
\usepackage{graphicx}
\usepackage[thinlines]{easytable}
\usepackage{array}

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2023} with \usepackage[nohyperref]{icml2023} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
\usepackage[accepted]{icml2023}

% If accepted, instead use the following line for the camera-ready submission:
% \usepackage[accepted]{icml2023}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{LLM Prompt Recovery}

\begin{document}


\twocolumn[

\centering EECS 230 Deep Learning Midterm Project Report


\icmltitle{LLM Prompt Recovery}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2023
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Kevin Chau}{equal,school}
\icmlauthor{Dhawal Modi}{equal,school}
\end{icmlauthorlist}

\icmlaffiliation{school}{Department of EECS, University of California, Merced, USA}

\icmlcorrespondingauthor{Kevin Chau}{kchau15@ucmerced.edu}
\icmlcorrespondingauthor{Dhawal Modi}{dmodi2@ucmerced.edu}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Deep Learning, Large Language Models, LLM, Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
Large Language Models (LLMs) are increasingly used to format and write texts in stylistic ways. The goal of this project is to recover the LLM prompt that was used to transform and rewrite a given text. This project is hosted on Kaggle and the model will be evaluated with an unseen test dataset generated by Gemma, Google's new family of open models. As a preliminary result, we submitted a pre-trained Mixtral 8x7B model to the Kaggle competition and received a Cosine Similarity score of 0.57 across all prompt predictions.
\end{abstract}

\section{Introduction}
The relatively recent of advent of publicly accessible Generative AI tools such as ChatGPT \cite{chatgpt} has created an explosion in the general application of deep learning. We are now entering a new AI age where users can leverage Large Language Models to assist with everyday creative writing tasks such as generating essays, emails, articles, and stories, all with professional and authentic quality.

New LLMs with increasing capabilities are being released every quarter, but there is little literature on how to effectively prompt an LLM. This has led to the process of "Prompt Engineering", which studies how to tune prompts to get the best response from an LLM. The goal of this project is to create a model that can recover the LLM prompt that was used to rewrite a given text. By focusing on the inverse problem of \textbf{Prompt Recovery}, we hope to develop a deeper understanding of AI driven NLP and LLMs.   

Our prompt recovery model will be tested on the Kaggle platform (via a public code competition) against a dataset of over 1300 original texts paired with a rewritten version from Gemma, Google's new family of open models.

\section{Background and Related Work}
\subsection{Google Gemma Open LLM}
 Earlier this year, Google AI released \textbf{Gemma} \cite{geminiteam2023gemini}\cite{googlegemma}, a family of new state-of-the-art open models based on the same technology as Google's Gemini LLM. Unlike Gemini, their largest and most capable AI application currently available, Gemma models are open-source and comparatively lightweight, as they are tailored for software developers and machine learning researchers.

Gemma model weights have been released in two sizes: Gemma 2B and Gemma 7B. JAX, Pytorch, and Tensorflow backends are supported through Keras, which enables a large variety of software toolchains for development. Gemma has readily available integrations with Colab, Kaggle, Hugging Face, TensorRT-LLM, Vertex AI, and Google Kubernetes Engine. It is highly optimized for different AI hardware platforms including NVIDIA GPUs and Google Cloud TPUs. For these reasons, we believe studying prompt recovery with Gemma is highly valuable and can have far reaching impacts in AI research.

\subsection{Prompt Engineering and Recovery}
The problem of how to best interact with AI models to generate the most desirable and accurate output text is relatively new and still being actively researched. \textbf{LLM Prompt Recovery} can help us gain more insights about how to accurately craft prompts to get the best outputs from an Instruction-tuned LLM. While there have been virtually no studies on prompt recovery, RewriteLM \cite{shu2023rewritelm} talks about how an Instruction-Tuned Language Model for text rewriting tackles the problem of generating rewritten texts through a special prompt.

\subsection{Dataset}
The dataset for the competition consists of a tuple of the following triplets, $\langle \text{\textit{Original Text}} \rangle$, $\langle \text{\textit{Rewrite Prompt}} \rangle$, $\langle \text{\textit{Rewritten Text}} \rangle$. We generated our own synthetic dataset using the Gemma \cite{googlegemma} model with 7 billion parameters. The model was instruction-tuned. The text corpus for $\langle \text{\textit{Original Text}} \rangle$ was obtained from Wikipedia-Movie-Plots and Kaggle forum messages \cite{jim_plotts_megan_risdal_2023}.

% \subsection{Related Work}
% List some related work on Prompt Engineering and Prompt Recovery here.

\section{Proposal} % NOTE: Maybe swap this with the background section
The goal of this project is to predict a prompt, $\langle \text{\textit{Rewrite Prompt}} \rangle$, that was used to generate $\langle \text{\textit{Rewritten Text}} \rangle$ by providing only $\langle \text{\textit{Original Text}} \rangle$-$\langle \text{\textit{Rewritten} Text} \rangle$ pairs to the LLM. To tackle this challenge of prompt recovery we propose training a DeBERTa base model \cite{he2021deberta} that minimizes cosine similarity between the predicted embedding and the true embedding from a \textit{sentence-t5} model \cite{ni2021sentencet5}. Then at inference, it predicts an embedding using the $\langle \text{\textit{Original Text}} \rangle$ and the $\langle \text{\textit{Rewritten Text}} \rangle$. This embedding is compared against a knowledge database of embeddings. The most similar embedding in this database is our model prediction. The score for each predicted / expected pair is calculated using the Sharpened Cosine Similarity, using an exponent of 3.

\section{Preliminary Results} 

\subsection{Motivating Example}
To make clear what we mean by \textbf{prompt recovery} we will start with a simple motivating example for demonstration. Suppose we have some \textbf{original text} that we want an LLM to \textit{transform} such as the following movie plot summary, which was taken from the Wikipedia Movie Plots database:

\begin{displayquote}
"Scenes are introduced using lines of the poem. Santa Claus, played by Harry Eytinge, is shown feeding real reindeer and finishes his work in the workshop. Meanwhile, the children of a city household hang their stockings and go to bed..."
\end{displayquote}

We may ask any well-trained LLM to \textbf{rewrite} the text for us, using a stylistic \textbf{prompt} or specific guidance for how we want the \textbf{rewritten text} to appear. Some simple prompts could be: "Make this rhyme", "Make this shorter", "Make this longer", "Rewrite this as a poem in the style of William Shakespeare".

\subsection{Rewriting Text with Gemma}
We wrote a python notebook to programmatically interact with the Gemma 7B model, since there is currently no user-friendly GUI-based web application widely available. 

Continuing with our running example, we asked Gemma to take the plot summary of \textit{The Night Before Christmas (1905)} and \textbf{"Make it rhyme"}. The pre-trained model produced the following rewritten text:

\begin{displayquote}
"Sure, here's the rhyme: Scenes dance with lines of rhyme, Santa's visit, a wondrous chime. He feeds reindeer, a festive sight, And finishes his work, day and night..."
\end{displayquote}

With our python notebook, we asked Gemma to process a set of original texts extracted from several databases with randomly picked prompts from a handmade list. Using this method, we are able to generate datasets of arbitrary length for testing and training (see section 2.3).

\subsection{Baseline prompt prediction using existing LLMs}
For a performance baseline, we asked several current LLMs to make prompt predictions given an original text and rewritten text pair. They were not given any knowledge of the original prompt. We tried several smaller LLMs such as Gemma itself and Mixtral \cite{jiang2024mixtral}, as well as larger production models including ChatGPT, Gemini, and Claude. Specifically, we queried the LLMs in this format: 
\begin{displayquote}
\textbf{"Original Text: $\langle \text{\textit{Original Text}} \rangle$ Rewritten Text: $\langle \text{\textit{Rewritten Text}} \rangle$ What was the specific rewrite prompt for this pair of text? Limit output to one line."}
\end{displayquote}

For the \textit{Night Before Christmas} example, they produced the following prompts:

\begin{center}
\begin{tabular}{ | m{3.5em} | m{18em}| } 
\hline
  \textbf{\textit{LLM}} & \textbf{\textit{Predicted Prompt}} \\ 
  \hline
  \textbf{Gemini} & Rewrite the passage in a rhyming and poetic style, maintaining the original story elements. \\ 
  \hline
  \textbf{Claude Sonnet} & Convert the given prose description into a rhyming poem. \\ 
  \hline
  \textbf{ChatGPT 3.5} & The specific rewrite prompt is not provided, so the details of the instructions for generating the rewritten text in one line cannot be determined. \\ 
  \hline
  \textbf{Gemma 2B} & A. to bring the poem to life B. to make the poem more interesting C. to make the poem more understandable...  \\ 
  \hline
  \textbf{Mixtral 8x7B} & Transform the original essay into a rhymed poem, preserving the key events while adding poetic language and meter. \\ 
  \hline
\end{tabular}
\end{center}

Gemini, Claude, and Mixtral inferred prompts that correctly identify the rhyming instruction, but fail to reproduce the exact prompt with respect to brevity and specific language. Interestingly, ChatGPT failed to comprehend our prompt recovery query and Gemma 2B was perhaps too small to sufficiently understand the task.

\textbf{It is clear that using existing pre-trained models is not sufficient for the task of prompt recovery. We aim to develop a much better solution to this problem by training our own LLMs as outlined in the proposal section.}

\section*{Software and Data Availability}
All of the results and software used to develop our models are publicly available and can be found on GitHub here: \url{https://github.com/kevin-chau/llm-prompt-recovery}

\nocite{llm-prompt-recovery}
\nocite{roberts2022scaling}
\nocite{mixtral}
\bibliography{example_paper}
\bibliographystyle{icml2023}


\end{document}
