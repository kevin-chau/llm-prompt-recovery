@misc{llm-prompt-recovery,
    author = {Will Lifferth, Paul Mooney, Sohier Dane, Ashley Chow},
    title = {LLM Prompt Recovery},
    publisher = {Kaggle},
    year = {2024},
    url = {https://kaggle.com/competitions/llm-prompt-recovery}
}

@Article{geminiteam2023gemini,
      title={Gemini: A Family of Highly Capable Multimodal Models}, 
      author={{Gemini Team Google}},
      year={2023},
      journal={arXiv:2312.11805 [cs.CL]},
      eprint={2312.11805},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@online{googlegemma,
      title={Gemma: A family of lightweight, state-of-the art open models built from the same research and technology used to create the Gemini models}, 
      author={{Google AI}},
      year={2024},
      url={https://ai.google.dev/gemma},
}

@misc{roberts2022scaling,
      title={Scaling Up Models and Data with $\texttt{t5x}$ and $\texttt{seqio}$}, 
      author={Adam Roberts and Hyung Won Chung and Anselm Levskaya and Gaurav Mishra and James Bradbury and Daniel Andor and Sharan Narang and Brian Lester and Colin Gaffney and Afroz Mohiuddin and Curtis Hawthorne and Aitor Lewkowycz and Alex Salcianu and Marc van Zee and Jacob Austin and Sebastian Goodman and Livio Baldini Soares and Haitang Hu and Sasha Tsvyashchenko and Aakanksha Chowdhery and Jasmijn Bastings and Jannis Bulian and Xavier Garcia and Jianmo Ni and Andrew Chen and Kathleen Kenealy and Jonathan H. Clark and Stephan Lee and Dan Garrette and James Lee-Thorp and Colin Raffel and Noam Shazeer and Marvin Ritter and Maarten Bosma and Alexandre Passos and Jeremy Maitin-Shepard and Noah Fiedel and Mark Omernick and Brennan Saeta and Ryan Sepassi and Alexander Spiridonov and Joshua Newlan and Andrea Gesmundo},
      year={2022},
      eprint={2203.17189},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@online{chatgpt,
      title={{ChatGPT}}, 
      author={{OpenAI}},
      year={2024},
      url={https://chat.openai.com/chat},
}

@misc{shu2023rewritelm,
      title={RewriteLM: An Instruction-Tuned Large Language Model for Text Rewriting}, 
      author={Lei Shu and Liangchen Luo and Jayakumar Hoskere and Yun Zhu and Yinxiao Liu and Simon Tong and Jindong Chen and Lei Meng},
      year={2023},
      eprint={2305.15685},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{jim_plotts_megan_risdal_2023,
	title={Meta Kaggle Code},
	url={https://www.kaggle.com/ds/3240808},
	DOI={10.34740/KAGGLE/DS/3240808},
	publisher={Kaggle},
	author={Jim Plotts and Megan Risdal},
	year={2023}
}

@misc{he2021deberta,
      title={DeBERTa: Decoding-enhanced BERT with Disentangled Attention}, 
      author={Pengcheng He and Xiaodong Liu and Jianfeng Gao and Weizhu Chen},
      year={2021},
      eprint={2006.03654},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{ni2021sentencet5,
      title={Sentence-T5: Scalable Sentence Encoders from Pre-trained Text-to-Text Models}, 
      author={Jianmo Ni and Gustavo Hernández Ábrego and Noah Constant and Ji Ma and Keith B. Hall and Daniel Cer and Yinfei Yang},
      year={2021},
      eprint={2108.08877},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{jiang2024mixtral,
      title={Mixtral of Experts}, 
      author={Albert Q. Jiang and Alexandre Sablayrolles and Antoine Roux and Arthur Mensch and Blanche Savary and Chris Bamford and Devendra Singh Chaplot and Diego de las Casas and Emma Bou Hanna and Florian Bressand and Gianna Lengyel and Guillaume Bour and Guillaume Lample and Lélio Renard Lavaud and Lucile Saulnier and Marie-Anne Lachaux and Pierre Stock and Sandeep Subramanian and Sophia Yang and Szymon Antoniak and Teven Le Scao and Théophile Gervet and Thibaut Lavril and Thomas Wang and Timothée Lacroix and William El Sayed},
      year={2024},
      eprint={2401.04088},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@online{mixtral,
    title={{Mixtral of Experts}},
    author={{Mistral AI}},
    year={2024},
    url={https://mistral.ai/news/mixtral-of-experts/}
}