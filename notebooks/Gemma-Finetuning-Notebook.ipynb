{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "sourceId": 67121,
     "databundleVersionId": 7806901,
     "sourceType": "competition"
    },
    {
     "sourceId": 7731345,
     "sourceType": "datasetVersion",
     "datasetId": 4517764
    },
    {
     "sourceId": 7733314,
     "sourceType": "datasetVersion",
     "datasetId": 4518936
    },
    {
     "sourceId": 7747717,
     "sourceType": "datasetVersion",
     "datasetId": 4506214
    },
    {
     "sourceId": 7909486,
     "sourceType": "datasetVersion",
     "datasetId": 4646458
    },
    {
     "sourceId": 168211113,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 4298,
     "sourceType": "modelInstanceVersion",
     "modelInstanceId": 3093
    },
    {
     "sourceId": 4302,
     "sourceType": "modelInstanceVersion",
     "modelInstanceId": 3097
    },
    {
     "sourceId": 5112,
     "sourceType": "modelInstanceVersion",
     "modelInstanceId": 3900
    },
    {
     "sourceId": 5994,
     "sourceType": "modelInstanceVersion",
     "modelInstanceId": 4761
    },
    {
     "sourceId": 11394,
     "sourceType": "modelInstanceVersion",
     "modelInstanceId": 8332
    }
   ],
   "dockerImageVersionId": 30665,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "!wandb disabled"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-03-22T00:37:34.412384Z",
     "iopub.execute_input": "2024-03-22T00:37:34.413126Z",
     "iopub.status.idle": "2024-03-22T00:37:36.898819Z",
     "shell.execute_reply.started": "2024-03-22T00:37:34.413086Z",
     "shell.execute_reply": "2024-03-22T00:37:36.897723Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from accelerate import Accelerator\n",
    "\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\n",
    "\n",
    "from datasets import Dataset"
   ],
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.status.busy": "2024-03-22T00:37:36.900826Z",
     "iopub.execute_input": "2024-03-22T00:37:36.901130Z",
     "iopub.status.idle": "2024-03-22T00:37:54.600018Z",
     "shell.execute_reply.started": "2024-03-22T00:37:36.901102Z",
     "shell.execute_reply": "2024-03-22T00:37:54.599024Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import datetime\n",
    "start_time = datetime.datetime.now()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-03-22T00:37:54.601433Z",
     "iopub.execute_input": "2024-03-22T00:37:54.602201Z",
     "iopub.status.idle": "2024-03-22T00:37:54.606284Z",
     "shell.execute_reply.started": "2024-03-22T00:37:54.602173Z",
     "shell.execute_reply": "2024-03-22T00:37:54.605399Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-03-22T00:37:54.608790Z",
     "iopub.execute_input": "2024-03-22T00:37:54.609053Z",
     "iopub.status.idle": "2024-03-22T00:37:54.642175Z",
     "shell.execute_reply.started": "2024-03-22T00:37:54.609031Z",
     "shell.execute_reply": "2024-03-22T00:37:54.641196Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, AutoConfig\n",
    "\n",
    "accelerator = Accelerator()\n",
    "\n",
    "# Comment/Uncomment and use as per wish\n",
    "#access_token = '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'\n",
    "#MODEL_PATH = \"../models/gemma_hf/gemma-7b-it\"\n",
    "MODEL_PATH = \"../models/gemma_hf/gemma-2b-it\"\n",
    "\n",
    "# quantization_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit = True,\n",
    "#     bnb_4bit_quant_type=\"nf4\",\n",
    "#     bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "#     bnb_4bit_use_double_quant=True,\n",
    "# )\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     MODEL_PATH,\n",
    "#     device_map = \"auto\",\n",
    "#     trust_remote_code = True,\n",
    "#     quantization_config=quantization_config,\n",
    "# )\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    device_map = \"auto\",\n",
    "    trust_remote_code = True\n",
    ")\n",
    "\n",
    "model = accelerator.prepare(model)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-03-22T00:37:54.643299Z",
     "iopub.execute_input": "2024-03-22T00:37:54.643624Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# TEST_DF_FILE = '/kaggle/input/llm-prompt-recovery/test.csv'\n",
    "TEST_DF_FILE = '../data/test.csv'\n",
    "SUB_DF_FILE = '../data/sample_submission.csv'\n",
    "NROWS = 1\n",
    "\n",
    "tdf = pd.read_csv(TEST_DF_FILE, nrows=NROWS, usecols=['id', 'original_text', 'rewritten_text'])\n",
    "sub = pd.read_csv(SUB_DF_FILE, nrows=NROWS, usecols=['id', 'rewrite_prompt'])"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def truncate_txt(text, length):\n",
    "    text_list = text.split()\n",
    "    \n",
    "    if len(text_list) <= length:\n",
    "        return text\n",
    "    \n",
    "    return \" \".join(text_list[:length])\n",
    "\n",
    "\n",
    "def gen_prompt(og_text, rewritten_text):\n",
    "    \n",
    "    og_text = truncate_txt(og_text, 200)\n",
    "    rewritten_text = truncate_txt(rewritten_text, 200)\n",
    "    \n",
    "    return (f\"Given below are 2 texts, the Rewritten text was created from the Original text using the google Gemma model. You are trying to understand how the original text was transformed into a new version. Analyze the changes in style and theme and come up with a prompt that must have been used to transform the Original Text to the Rewritten Text. Start your output with the string \\\"Prompt:\\\". Also limit your output to just the rewrite prompt. Do not include the original text and the rewritten text in your response.\\n\"\n",
    "            f\"\"\"Original Text:\\\"\"\"{og_text}\\\"\"\"\\nRewritten Text:\\\"\"\"{rewritten_text}\\\"\"\"\\n\"\"\")"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# QLoRA Fine-tuning"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "model = prepare_model_for_kbit_training(model)\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    r=64,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\"gate_proj\"]\n",
    ")\n",
    "model = get_peft_model(model, peft_config)"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "training_arguments = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=1,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    save_steps=25,\n",
    "    logging_steps=25,\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.001,\n",
    "    fp16=False,\n",
    "    bf16=False,\n",
    "    max_grad_norm=0.3,\n",
    "    max_steps=-1,\n",
    "    warmup_ratio=0.03,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    report_to=\"wandb\"\n",
    ")"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# `LLM Prompt Recovery - Synthetic Datastore dataset` by @dschettler8845\n",
    "df1 = pd.read_csv(\"../data/other/gemma1000_w7b.csv\")\n",
    "df1 = df1[[\"original_text\", \"rewrite_prompt\", \"gemma_7b_rewritten_text_temp0\"]]\n",
    "df1 = df1.rename(columns={\"gemma_7b_rewritten_text_temp0\":\"rewritten_text\"})\n",
    "df1.head(2)\n",
    "\n",
    "# `3000 Rewritten texts - Prompt recovery Challenge` by @dipamc77\n",
    "df2 = pd.read_csv(\"../data/other/prompts_0_500_wiki_first_para_3000.csv\")\n",
    "df2.head(2)\n",
    "\n",
    "# Merge all datasets\n",
    "df = pd.concat([df1, df2], axis=0)\n",
    "df = df.sample(4000).reset_index(drop=True) # to reduce training time we are only using 2k samples\n",
    "df.head(5)"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "dict = {\n",
    "    \"id\": [str(x) for x in range(0,len(df))],\n",
    "    \"original_text\": [str(x) for x in df.original_text],\n",
    "    \"rewrite_prompt\": [str(x) for x in df.rewrite_prompt],\n",
    "    \"rewritten_text\": [str(x) for x in df.rewritten_text],\n",
    "}\n",
    "dict['rewritten_text'][6]"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "ds = Dataset.from_dict(dict)\n",
    "ds[0].keys()"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def truncate_text(text, max_len): #without this    \n",
    "    text_split = text.split()    \n",
    "    if len(text_split)<=max_len:\n",
    "        return text\n",
    "    else:\n",
    "        return \" \".join(text_split[:max_len])\n",
    "\n",
    "def tokenize(example):\n",
    "    for ot, rt, rp in zip(example[\"original_text\"], example[\"rewritten_text\"], example[\"rewrite_prompt\"]):\n",
    "        # original_text = truncate_text(ot, 200)\n",
    "        # rewritten_text = truncate_text(rt, 200)\n",
    "        # rewrite_prompt = truncate_text(rp, 100)\n",
    "        \n",
    "        original_text = example[\"original_text\"]\n",
    "        rewritten_text = example[\"rewritten_text\"]\n",
    "        rewrite_prompt = example[\"rewrite_prompt\"]\n",
    "        \n",
    "        # template = f\"\"\"Original Text:\\\"\"\"{original_text}\\\"\"\"\\nRewritten Text:\\\"\"\"{rewritten_text}\\\"\"\"\\nGiven are 2 texts, the Rewritten text was created from the Original text using the google Gemma model. You are trying to understand how the original text was transformed into a new version. Analyzing the changes in style, theme, etc., please come up with a prompt that must have been used to guide the transformation from the original to the rewritten text. Start directly with the prompt, that's all I need. Output should be one line ONLY.\\n\\nPrompt:\\n\\\"{rewrite_prompt}\\\"\"\"\"\n",
    "        \n",
    "        template = f\"Given below are 2 texts, the Rewritten text was created from the Original text using the Google Gemma model. You are trying to understand how the original text was transformed into a new version. Analyze the changes in style and theme and come up with a prompt that must have been used to transform the Original Text to the Rewritten Text. Start your output with the string \\\"Prompt:\\\". Also limit your output to just the rewrite prompt. Do not include the original text and the rewritten text in your response.\\n\"\n",
    "        f\"\"\"Original Text:\\\"\"\"{original_text}\\\"\"\"\\nRewritten Text:\\\"\"\"{rewritten_text}\\\"\"\"\\nPrompt:\\n\\\"{rewrite_prompt}\\\"\"\"\"\n",
    "        \n",
    "        tkn = tokenizer(template, padding=True)\n",
    "    return {**tkn}\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    \n",
    "ds_tokenize = ds.map(tokenize)\n",
    "ds_tokenize[0].keys()"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "ds_split = ds_tokenize.train_test_split(test_size=0.20)\n",
    "\n",
    "train_ds = ds_split['train']\n",
    "test_ds = ds_split['test']"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import Trainer, DataCollatorForLanguageModeling\n",
    "trainer = Trainer(\n",
    "    model=model, # lora enabled\n",
    "    train_dataset=ds_tokenize,    \n",
    "    args=TrainingArguments(\n",
    "        per_device_train_batch_size=1,\n",
    "        learning_rate=2e-5,\n",
    "        fp16=True,\n",
    "        output_dir=\".\",\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        num_train_epochs=1\n",
    "    ),\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    "    #compute_metrics=compute_metrics,\n",
    ")"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "trainer.train()"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model_path = \"gemma_2b_finetuned_model\"\n",
    "model.save_pretrained(model_path)\n",
    "tokenizer.save_pretrained(model_path)"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "shutil.make_archive(\"gemma-twob-it-finetuned\", \"zip\", model_path)"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "trainer.tokenizer.save_pretrained(\"finetuned_model\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "trainer.model.save_pretrained(\"finetuned_model\")"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "merged_model= PeftModel.from_pretrained(model, \"finetuned_model\")\n",
    "merged_model= merged_model.merge_and_unload()\n",
    "\n",
    "# Save the merged model\n",
    "merged_model.save_pretrained(\"merged_model\",safe_serialization=True)\n",
    "tokenizer.save_pretrained(\"merged_model\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Test / Validate Fine-tuning"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# test_data = pd.read_csv(open(\"/kaggle/input/combined/test2.csv\"))\n",
    "# test_data"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# test_ds = Dataset.from_dict({\n",
    "#     \"id\": [str(x) for x in range(0,len(df))],\n",
    "#     \"original_text\": [x for x in df.original_text],\n",
    "#     \"rewritten_text\": [x for x in df.rewritten_text],\n",
    "# })\n",
    "# test_ds"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# def tokenize_test(example):\n",
    "#     for ot, rt in zip(example[\"original_text\"], example[\"rewritten_text\"]):\n",
    "#         original_text = truncate_text(ot, 200)\n",
    "#         rewritten_text = truncate_text(rt, 200)\n",
    "    \n",
    "#         template = f\"Instruction:\\nGiven 2 texts: Original Text and Rewritten Text. An LLM is received a prompt from a user asking to rewrite the given Original Text. Based on this prompt, the LLM generated the given Rewritten Text from the given Original Text in a certain way specified in the prompt. Your task is to guess the prompt with which the LLM generated the Rewritten Text.\\nOriginal Text: \\\"{original_text}\\\".\\nRewritten Text: \\\"{rewritten_text}\\\".\\n\\nResponse:\\n\\\"\\\"\"\n",
    "#         tkn = tokenizer(template, padding=True, return_tensors=\"pt\")\n",
    "#     return {**tkn}\n",
    "    \n",
    "# test_ds_tokenized = test_ds.map(tokenize_test)\n",
    "# test_ds_tokenized[0].keys()"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# model = model.to(device)\n",
    "\n",
    "# for item in test_ds:\n",
    "#     print(item) # only 1 item so its okay to print out   \n",
    "#     inputs = tokenize_test(item).to(device)\n",
    "#     print(inputs)\n",
    "    \n",
    "#     #model.config.use_cache = False  # silence the warnings.\n",
    "#     outputs = model.generate(**inputs)\n",
    "#     print(outputs)\n",
    "\n",
    "#     text_outputs = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "#     print('\\n\\n', text_outputs)\n"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# text = \"Improve the text\"\n",
    "# inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "# print(inputs)\n",
    "\n",
    "# model = model.to(device)\n",
    "# model.config.use_cache = False  # silence the warnings.\n",
    "# outputs = model.generate(**inputs)\n",
    "# print(outputs)\n",
    "\n",
    "# print('\\n\\n', tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Prediction"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "TEST_DF_FILE = '../data/combined/val.csv'\n",
    "#TEST_DF_FILE = '../data/test.csv'\n",
    "SUB_DF_FILE = '../data/sample_submission.csv'\n",
    "NROWS = 5\n",
    "\n",
    "tdf = pd.read_csv(TEST_DF_FILE, nrows=NROWS, usecols=['id', 'original_text', 'rewritten_text'])\n",
    "sub = pd.read_csv(SUB_DF_FILE, nrows=NROWS, usecols=['id', 'rewrite_prompt'])"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer,  BitsAndBytesConfig, AutoConfig\n",
    "from peft import PeftModel\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from accelerate import Accelerator\n",
    "\n",
    "accelerator = Accelerator()\n",
    "device = accelerator.device\n",
    "\n",
    "# quantization_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit = True,\n",
    "#     bnb_4bit_quant_type=\"nf4\",\n",
    "#     bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "#     bnb_4bit_use_double_quant=True,\n",
    "# )\n",
    "\n",
    "model_path = \"gemma_2b_finetuned_model\"\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_path,\n",
    "#     device_map = \"auto\",\n",
    "#     quantization_config=quantization_config)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    device_map = \"auto\",\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "model = PeftModel.from_pretrained(model, model_path)"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import gc\n",
    "import re\n",
    "\n",
    "#model = accelerator.prepare(model)\n",
    "\n",
    "tdf['id'] = sub['id'].copy()\n",
    "\n",
    "pbar = tqdm(total=tdf.shape[0])\n",
    "\n",
    "it = iter(tdf.iterrows())\n",
    "idx, row = next(it, (None, None))\n",
    "\n",
    "# https://www.kaggle.com/competitions/llm-prompt-recovery/discussion/481116\n",
    "DEFAULT_TEXT = \"Please improve the following text using the writing style of, maintaining the original meaning but altering the tone, diction, and stylistic elements to match the new style.Enhance the clarity, elegance, and impact of the following text by adopting the writing style of , ensuring the core message remains intact while transforming the tone, word choice, and stylistic features to align with the specified style.\"\n",
    "\n",
    "res = []\n",
    "\n",
    "\n",
    "while idx is not None:\n",
    "    \n",
    "    if (datetime.datetime.now() - start_time) > datetime.timedelta(hours=8, minutes=30):\n",
    "        res.append([row[\"id\"], DEFAULT_TEXT])\n",
    "        idx, row = next(it, (None, None))\n",
    "        pbar.update(1)\n",
    "        continue\n",
    "        \n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "        \n",
    "    try:        \n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": gen_prompt(row[\"original_text\"], row[\"rewritten_text\"])\n",
    "            }\n",
    "        ]\n",
    "        encoded_input = tokenizer.apply_chat_template(messages, return_tensors=\"pt\", add_generation_prompt=True).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            encoded_output = model.generate(encoded_input, max_new_tokens=100, do_sample=True, pad_token_id=tokenizer.eos_token_id)\n",
    "        \n",
    "        decoded_output = tokenizer.batch_decode(encoded_output, skip_special_tokens=True)[0]\n",
    "        \n",
    "        decoded_output = re.sub(r\"[\\s\\S]*model\", 'model', decoded_output, 1)\n",
    "\n",
    "        # Updated regex to capture everything after 'model' leading up to the quoted prompt\n",
    "        match = re.search(r'Prompt:s*(.+)', decoded_output, re.DOTALL)\n",
    "        if match is not None:\n",
    "            prompt = match.group(1)\n",
    "            #print(prompt)\n",
    "            res.append([row[\"id\"], prompt])\n",
    "        else:\n",
    "            res.append([row[\"id\"], DEFAULT_TEXT])\n",
    "                            \n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: {e}\")\n",
    "        res.append([row[\"id\"], DEFAULT_TEXT])\n",
    "        \n",
    "    finally:\n",
    "        idx, row = next(it, (None, None))\n",
    "        pbar.update(1)\n",
    "\n",
    "        \n",
    "pbar.close()"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# sub[\"rewrite_prompt\"] = tdf['rewrite_prompt'].copy()\n",
    "# sub.to_csv(\"submission.csv\", index=False)\n",
    "sub = pd.DataFrame(res, columns=['id', 'rewrite_prompt'])\n",
    "\n",
    "sub.to_csv(\"sample_submission.csv\", index=False)\n",
    "sub.to_csv(\"submission.csv\", index=False)"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "sub"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "res"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ]
}
